{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k_WMvNEEg8ut"
   },
   "source": [
    "# Neural Network Training Techniques\n",
    "\n",
    "## Batch Normalization Study\n",
    "\n",
    "This notebook demonstrates the impact of batch normalization on neural network training by comparing two architectures:\n",
    "1. A baseline feed-forward network\n",
    "2. The same architecture enhanced with batch normalization layers\n",
    "\n",
    "Both networks are trained on the FashionMNIST dataset to empirically compare convergence speed and final performance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qr38k5SKg8uu"
   },
   "source": [
    "## Batch Normalization\n",
    "In this study we construct a feed forward neural network with batch normalization during training. Training deep neural networks can be challenging due to the change in the distribution of inputs to layers deep in the network as a result of the updates of the weights in the previous layers. This causes the learning to chase a \"moving target\", which slows down the learning process. Batch normalization is a technique that aims to address this problem by normalizing layer inputs. This stabilizes the learning process and can greatly decrease training time. If you are interested you can read the paper introducing batch normalization [here](https://arxiv.org/abs/1502.03167).\n",
    "\n",
    "We will be working with the [FashionMNIST](https://pytorch.org/vision/stable/generated/torchvision.datasets.FashionMNIST.html) dataset by Zalando, which consists of $28\\times 28$ black and white images and has 10 classes just like the [MNIST](https://pytorch.org/vision/main/generated/torchvision.datasets.MNIST.html) dataset. But instead of numbers the classes are various items of clothing such as shoes, t-shirts, dresses, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gpULKJZgg8uv"
   },
   "source": [
    "### Baseline Network\n",
    "We construct a feed forward neural network with 3 hidden linear layers with a ReLU after each of the first 2 layers. The first layer has a hidden size of 64 and the second a hidden size of 32. This is a multi-class classification problem so we use cross entropy loss (provided by [nn.CrossEntropyLoss](https://pytorch.org/docs/stable/generated/torch.nn.CrossEntropyLoss.html)). We use stochastic gradient descent (provided by [torch.optim.SGD](https://pytorch.org/docs/stable/generated/torch.optim.SGD.html)) with a learning rate of 0.001 as the optimizer. The network is trained on the training data for 5 epochs and accuracy on the **test** set is reported after each epoch, for this refer to [PyTorch Training Loop](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#the-training-loop) and [Per-Epoch Activity](https://pytorch.org/tutorials/beginner/introyt/trainingyt.html#per-epoch-activity).\n",
    "\n",
    "**Note**: The data comes in the format of $28 \\times 28$ tensors, so we flatten it before training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2pA0YYKHg8uw"
   },
   "source": [
    "### Network with Batch Normalization\n",
    "We construct another network with the same parameters as the baseline but this time include a batch normalization layer (refer to [nn.BatchNorm1d](https://pytorch.org/docs/stable/generated/torch.nn.BatchNorm1d.html)) after the hidden layers before the activation function. Train the network and save the test accuracies."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LzguVC1qg8uw"
   },
   "source": [
    "### Performance Comparison\n",
    "We plot the accuracies of the two networks and analyze the results."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "nnti",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  },
  "colab": {
   "provenance": []
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}