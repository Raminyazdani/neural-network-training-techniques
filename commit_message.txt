Ramin Yazdani | Neural Network Training Techniques | main | WIP(batchnorm): Implement batch normalization enhanced network

Added NeuralNetworkWithBatchNorm class to implement the experimental batch normalization
enhancement. MISTAKE: Placed BatchNorm1d layers BEFORE Linear layers instead of after,
which is incorrect architecture that defeats the purpose of batch normalization.

After establishing baseline functionality, the developer implements the experimental
enhancement. However, this commit contains a realistic mistake - the BatchNorm layers
are positioned incorrectly in the forward pass. This would be caught during code review
or when comparing against PyTorch best practices.

WHY THIS IS WRONG:
- BatchNorm should normalize the linear transformation outputs, not the inputs
- Current order means bn1 normalizes raw input features (already normalized)
- This defeats the purpose of batch normalization in addressing internal covariate shift
- The order should be: Linear → BatchNorm → ReLU, not BN → Linear → ReLU

Changes:
- Added NeuralNetworkWithBatchNorm class
- Incorrectly placed BatchNorm1d layers BEFORE Linear layers
- Forward pass: BN → Linear → ReLU (incorrect order)

Files: 4 (notebook updated with incorrect BatchNorm network)

NOTE: This mistake will be corrected in the next commit (hotfix).
