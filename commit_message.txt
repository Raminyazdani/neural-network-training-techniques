Ramin Yazdani | Neural Network Training Techniques | main | WIP(hotfix): Fix - Correct BatchNorm layer positioning after linear layers

HOTFIX: Corrected the BatchNorm layer positioning to follow best practices. BatchNorm
layers now appear AFTER linear layers and BEFORE activation functions, which is the
proper order for addressing internal covariate shift.

During code review, the developer noticed the BatchNorm placement was incorrect. This
hotfix corrects the architecture to follow best practices: apply batch normalization to
the output of linear transformations, before the activation function. This is the
standard pattern in modern deep learning architectures.

HOW NOTICED:
- Code review and reference to PyTorch documentation on proper BatchNorm usage
- Comparison with established architectures (ResNet, VGG, etc.)
- Understanding that BatchNorm normalizes activations, not raw inputs

RESOLUTION:
- Restructured the forward pass to apply BatchNorm after each linear layer but before
  the activation function
- Corrected order: Linear → BatchNorm → ReLU

Changes:
- FIXED BatchNorm layer positioning
- Corrected order: Linear → BatchNorm → ReLU (instead of BN → Linear → ReLU)
- Updated forward pass: flatten → fc1 → bn1 → ReLU → fc2 → bn2 → ReLU → fc3 → output
- Added network instantiation

Files: 4 (notebook fixed with correct BatchNorm network)
