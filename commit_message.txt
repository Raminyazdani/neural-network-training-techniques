Ramin Yazdani | Neural Network Training Techniques | main | WIP(training): Train both baseline and BatchNorm networks

Executed training runs for both the baseline and batch normalization networks using
identical training conditions (5 epochs, SGD optimizer, learning rate 0.001) to enable
fair comparison. Recorded training accuracies for both networks.

With both network architectures correctly implemented, the developer trains them under
identical conditions to enable fair comparison. This allows for objective evaluation of
the impact of batch normalization on convergence speed and final accuracy. Data is
prepared for the comparative analysis in the next step.

Changes:
- Trained baseline network (if not already trained)
- Trained BatchNorm network using same training loop
- Recorded training accuracies for both networks
- Prepared data for comparative analysis

Files: 4 (notebook updated with training for both networks)
