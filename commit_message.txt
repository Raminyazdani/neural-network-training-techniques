Ramin Yazdani | Neural Network Training Techniques | main | WIP(model): Implement baseline feed-forward neural network architecture

Defined the baseline NeuralNetwork class inheriting from nn.Module with a three-layer
feed-forward architecture (784 → 64 → 32 → 10) using ReLU activations. This establishes
the reference model for comparison against the batch normalization variant.

The core model architecture is implemented first. Starting with the baseline provides a
reference point for comparison. The class definition is separated from instantiation for
clarity and follows PyTorch best practices for model definition.

Changes:
- Defined NeuralNetwork class inheriting from nn.Module
- Added architecture: Input(784) → Hidden1(64) → Hidden2(32) → Output(10)
- Implemented three linear layers (fc1, fc2, fc3)
- Defined forward pass with flatten, ReLU activations

Files: 4 (notebook updated with network class)
